import os
import pandas as pd
from docx import Document
import pdfplumber
from PIL import Image, ImageEnhance, ImageFilter
import pytesseract
import streamlit as st
import faiss
import numpy as np
import cohere
from openai import OpenAI
from dotenv import load_dotenv
import sqlite3
import qrcode
from io import BytesIO
from datetime import datetime, timedelta, date, timezone
import pickle
import json
import re
from netmiko import ConnectHandler
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS as LangchainFAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA, LLMChain  # LLMChain ì¶”ê°€
from langchain.prompts import PromptTemplate
from google.oauth2 import service_account
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from langchain.globals import set_llm_cache
from langchain.cache import SQLiteCache

# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="ìœ ì§€ë³´ìˆ˜ ì±—ë´‡", layout="wide")

# LangChainì˜ LLM í˜¸ì¶œ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬ ë¹„ìš©ê³¼ ì†ë„ë¥¼ ìµœì í™”
set_llm_cache(SQLiteCache(database_path="langchain_llm_cache.sqlite"))

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# --- ìƒìˆ˜ ë° ì„¤ì • ---
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
COHERE_API_KEY = os.getenv('COHERE_API_KEY')

if not OPENAI_API_KEY or not COHERE_API_KEY:
    st.error("OPENAI_API_KEY ë˜ëŠ” COHERE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# í´ë” ë° íŒŒì¼ ê²½ë¡œ
UPLOAD_FOLDER = 'Uploads'
ASSET_DB_PATH = 'assets.db'
GOOGLE_CREDS_PATH = 'credentials.json'
GOOGLE_TOKEN_PATH = 'token.pickle'
FAISS_INDEX_PATH = 'faiss_index.bin'
FAISS_METADATA_PATH = 'faiss_metadata.pkl'
NETWORK_RAW_DATA_FILE = "network_raw_data.txt"
NETWORK_FAISS_INDEX_PATH = "network_faiss_index"

# í—ˆìš©ëœ íŒŒì¼ í™•ì¥ì ë° ìµœëŒ€ íŒŒì¼ í¬ê¸° (16MB)
ALLOWED_EXTENSIONS = {'csv', 'docx', 'pdf', 'jpg', 'jpeg', 'png', 'ppt', 'pptx', 'txt'}
MAX_FILE_SIZE = 16 * 1024 * 1024

# ë„¤íŠ¸ì›Œí¬ ì±—ë´‡ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
NETWORK_PROMPT_TEMPLATE = """
### ì—­í•  ë° ëª©í‘œ
ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì œê³µëœ 'ì»¨í…ìŠ¤íŠ¸'(Cisco ì¥ë¹„ì˜ ë‹¤ì–‘í•œ show ëª…ë ¹ì–´ ê²°ê³¼)ë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### í–‰ë™ ì§€ì¹¨
1. **ì •ë³´ ì¢…í•©:** ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì€ ì—¬ëŸ¬ ëª…ë ¹ì–´ ê²°ê³¼ì— í©ì–´ì ¸ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • VLANì˜ IP ì •ë³´ë¥¼ ì•Œë ¤ë‹¬ë¼ëŠ” ìš”ì²­ì—ëŠ” `show vlan`, `show ip interface brief`, `show running-config`ì˜ ë‚´ìš©ì„ ëª¨ë‘ ì¢…í•©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
2. **ì •í™•í•œ ì •ë³´ ì¶”ì¶œ:** `show running-config`ì˜ `interface VlanX` ì„¹ì…˜ì—ì„œ `ip address [IP ì£¼ì†Œ] [ì„œë¸Œë„· ë§ˆìŠ¤í¬]` í˜•ì‹ì˜ ì •ë³´ë¥¼ ì •í™•íˆ ì°¾ì•„ë‚´ì„¸ìš”.
3. **êµ¬ì¡°í™”ëœ ë‹µë³€:** ê°€ëŠ¥í•˜ë‹¤ë©´, ì •ë³´ë¥¼ í‘œ(Markdown í…Œì´ë¸” í˜•ì‹)ë¡œ ì •ë¦¬í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”.
4. **ì¶”ë¡  ë° ìš”ì•½:** ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•˜ì§€ ë§ê³ , ì§ˆë¬¸ì˜ ì˜ë„ì— ë§ê²Œ ì •ë³´ë¥¼ ìš”ì•½í•˜ê³  ì¬êµ¬ì„±í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
5. **ì œí•œëœ ì •ë³´ ë‚´ì—ì„œ ë‹µë³€:** ì£¼ì–´ì§„ 'ì»¨í…ìŠ¤íŠ¸'ì— ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, "ì œê³µëœ ë¡œê·¸ ì •ë³´ì—ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

### ì»¨í…ìŠ¤íŠ¸
{context}

### ì§ˆë¬¸
{question}

### ë‹µë³€ (ìœ„ ì§€ì¹¨ì— ë”°ë¼ ìƒì„±):
"""
NETWORK_PROMPT = PromptTemplate(
    template=NETWORK_PROMPT_TEMPLATE, input_variables=["context", "question"]
)

# --- ì´ˆê¸°í™” ---
@st.cache_resource
def get_clients():
    co_client = cohere.Client(COHERE_API_KEY)
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
    dimension = 1024
    index = faiss.IndexFlatL2(dimension)
    metadata = []
    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_METADATA_PATH):
        try:
            index = faiss.read_index(FAISS_INDEX_PATH)
            with open(FAISS_METADATA_PATH, 'rb') as f:
                metadata = pickle.load(f)
            validated_metadata = []
            for item in metadata:
                if isinstance(item, dict) and 'text' in item:
                    validated_metadata.append(item)
                else:
                    st.warning(f"ì˜ëª»ëœ ë©”íƒ€ë°ì´í„° í•­ëª© ë°œê²¬: {item}. ë¬´ì‹œë©ë‹ˆë‹¤.")
            metadata = validated_metadata
            if index.ntotal != len(metadata):
                st.warning("FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ê¸¸ì´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                index = faiss.IndexFlatL2(dimension)
                metadata = []
                if os.path.exists(FAISS_INDEX_PATH):
                    os.remove(FAISS_INDEX_PATH)
                if os.path.exists(FAISS_METADATA_PATH):
                    os.remove(FAISS_METADATA_PATH)
        except Exception as e:
            st.warning(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            index = faiss.IndexFlatL2(dimension)
            metadata = []
    return co_client, index, openai_client, metadata

@st.cache_resource
def load_network_rag_chain():
    if not os.path.exists(NETWORK_FAISS_INDEX_PATH):
        return None
    try:
        embeddings = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta-multitask")
        vectorstore = LangchainFAISS.load_local(NETWORK_FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        retriever = vectorstore.as_retriever()
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=OPENAI_API_KEY)
        return RetrievalQA.from_chain_type(
            llm=llm, chain_type="stuff", retriever=retriever,
            return_source_documents=True, chain_type_kwargs={"prompt": NETWORK_PROMPT}
        )
    except Exception as e:
        st.error(f"ë„¤íŠ¸ì›Œí¬ RAG ì²´ì¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

co, index, openai_client, metadata = get_clients()

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

# --- ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìˆ˜ì§‘ ë° ì¸ë±ì‹± ---
def fetch_network_data(device_info, commands_to_run):
    all_output = ""
    try:
        with ConnectHandler(**device_info) as net_connect:
            net_connect.enable()
            net_connect.fast_cli = True  # fast_cli ëª¨ë“œ í™œì„±í™”
            for cmd in commands_to_run:
                # send_command_timingì„ ì‚¬ìš©í•˜ì—¬ ë” ë¹ ë¥¸ ì‘ë‹µì„ ê¸°ëŒ€
                output = net_connect.send_command_timing(cmd, delay_factor=1, max_loops=150, read_timeout=60)
                all_output += f"\n--- {cmd} ---\n{output}\n"
            net_connect.fast_cli = False # fast_cli ëª¨ë“œ ë¹„í™œì„±í™”
        with open(NETWORK_RAW_DATA_FILE, "w", encoding="utf-8") as f:
            f.write(all_output)
        return True, None
    except Exception as e:
        error_message = f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        return False, error_message

def build_network_vector_store():
    if not os.path.exists(NETWORK_RAW_DATA_FILE):
        error_message = f"{NETWORK_RAW_DATA_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”."
        return False, error_message
    try:
        loader = TextLoader(NETWORK_RAW_DATA_FILE, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
        docs = text_splitter.split_documents(documents)
        embeddings = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta-multitask")
        vectorstore = LangchainFAISS.from_documents(docs, embeddings)
        vectorstore.save_local(NETWORK_FAISS_INDEX_PATH)
        return True, None
    except Exception as e:
        error_message = f"ë„¤íŠ¸ì›Œí¬ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        return False, error_message

# --- LangChain ë‚ ì§œ ì¶”ì¶œ ì²´ì¸ ---
@st.cache_resource
def get_date_extraction_chain():
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=OPENAI_API_KEY)
    date_extraction_template = """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ë‚ ì§œ ë˜ëŠ” ê¸°ê°„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
- ì˜¤ëŠ˜ ë‚ ì§œëŠ” {today} ì…ë‹ˆë‹¤.
- 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì´ë²ˆ ì£¼' ë“± ìƒëŒ€ì  í‘œí˜„ì€ ì •í™•í•œ ë‚ ì§œë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
- ë‚ ì§œëŠ” 'YYYY-MM-DD' í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´, ì˜¤ëŠ˜ ë‚ ì§œë¥¼ start_dateì™€ end_dateë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
- ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œ(ì‚¬ëŒ ì´ë¦„, ì´ë²¤íŠ¸ ì¢…ë¥˜ ë“±)ë¥¼ 'query' í‚¤ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. í‚¤ì›Œë“œê°€ ì—†ê±°ë‚˜ 'ì „ì²´', 'ëª¨ë“ ' ë“±ì˜ ë‹¨ì–´ë§Œ ìˆìœ¼ë©´ 'ì „ì²´'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

---

**ì˜ˆì‹œ**
ì˜¤ëŠ˜ ë‚ ì§œ: 2025-08-01

ì§ˆë¬¸: 8ì›” 15ì¼ ì „ì²´ ì¼ì •ì„ ì•Œë ¤ì¤˜
JSON: {{"start_date": "2025-08-15", "end_date": "2025-08-15", "query": "ì „ì²´"}}

ì§ˆë¬¸: ë‹¤ìŒ ì£¼ ë°±ì¢…ìœ¤ì˜ ì¼ì •ì´ ì–´ë–»ê²Œ ë¼?
JSON: {{"start_date": "2025-08-04", "end_date": "2025-08-10", "query": "ë°±ì¢…ìœ¤"}}

ì§ˆë¬¸: í˜„ìš° ë¯¸íŒ…
JSON: {{"start_date": "2025-08-01", "end_date": "2025-08-01", "query": "í˜„ìš° ë¯¸íŒ…"}}

---

ì˜¤ëŠ˜ ë‚ ì§œ: {today}
ì§ˆë¬¸: {question}
JSON:"""
    date_extraction_prompt = PromptTemplate(template=date_extraction_template, input_variables=["today", "question"])
    return LLMChain(llm=llm, prompt=date_extraction_prompt)

date_extraction_chain = get_date_extraction_chain()

# --- SQLite ìì‚°ê´€ë¦¬ ê¸°ëŠ¥ ---
def init_asset_db():
    conn = sqlite3.connect(ASSET_DB_PATH)
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS assets (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            "ìì‚°ë²ˆí˜¸" TEXT NOT NULL UNIQUE,
            "ìœ„ì¹˜" TEXT,
            "ì¥ë¹„ìƒíƒœ" TEXT,
            "ì œì¡°ì‚¬" TEXT,
            "ì‹œë¦¬ì–¼" TEXT,
            "êµ¬ë¶„" TEXT,
            "ëª¨ë¸ëª…" TEXT,
            "ê¸°íƒ€" TEXT
        )
    ''')
    c.execute("PRAGMA table_info(assets)")
    columns = [info[1] for info in c.fetchall()]
    if "ì‹œìŠ¤í…œëª…" in columns and "ê¸°íƒ€" not in columns:
        c.execute('''
            CREATE TABLE assets_new (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                "ìì‚°ë²ˆí˜¸" TEXT NOT NULL UNIQUE,
                "ìœ„ì¹˜" TEXT,
                "ì¥ë¹„ìƒíƒœ" TEXT,
                "ì œì¡°ì‚¬" TEXT,
                "ì‹œë¦¬ì–¼" TEXT,
                "êµ¬ë¶„" TEXT,
                "ëª¨ë¸ëª…" TEXT,
                "ê¸°íƒ€" TEXT
            )
        ''')
        c.execute('''
            INSERT INTO assets_new (id, "ìì‚°ë²ˆí˜¸", "ìœ„ì¹˜", "ì¥ë¹„ìƒíƒœ", "ì œì¡°ì‚¬", "ì‹œë¦¬ì–¼", "êµ¬ë¶„", "ëª¨ë¸ëª…", "ê¸°íƒ€")
            SELECT id, "ìì‚°ë²ˆí˜¸", "ìœ„ì¹˜", "ì¥ë¹„ìƒíƒœ", "ì œì¡°ì‚¬", "ì‹œë¦¬ì–¼", "êµ¬ë¶„", "ëª¨ë¸ëª…", "ì‹œìŠ¤í…œëª…"
            FROM assets
        ''')
        c.execute("DROP TABLE assets")
        c.execute("ALTER TABLE assets_new RENAME TO assets")
    conn.commit()
    conn.close()

def add_asset(data):
    conn = sqlite3.connect(ASSET_DB_PATH)
    c = conn.cursor()
    try:
        c.execute('''
            INSERT INTO assets ("ìì‚°ë²ˆí˜¸", "ìœ„ì¹˜", "ì¥ë¹„ìƒíƒœ", "ì œì¡°ì‚¬", "ì‹œë¦¬ì–¼", "êµ¬ë¶„", "ëª¨ë¸ëª…", "ê¸°íƒ€")
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)''', (data['ìì‚°ë²ˆí˜¸'], data['ìœ„ì¹˜'], data['ì¥ë¹„ìƒíƒœ'], data['ì œì¡°ì‚¬'], data['ì‹œë¦¬ì–¼'], data['êµ¬ë¶„'], data['ëª¨ë¸ëª…'], data['ê¸°íƒ€']))
        conn.commit()
        return True
    except sqlite3.IntegrityError as e:
        st.error(f"DB ì‚½ì… ì˜¤ë¥˜: {e} (ìì‚°ë²ˆí˜¸: {data['ìì‚°ë²ˆí˜¸']})")
        return False
    except Exception as e:
        st.error(f"DB ì‚½ì… ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False
    finally:
        conn.close()

def get_assets():
    conn = sqlite3.connect(ASSET_DB_PATH)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    c.execute('SELECT id, "ìì‚°ë²ˆí˜¸", "ìœ„ì¹˜", "ì¥ë¹„ìƒíƒœ", "ì œì¡°ì‚¬", "ì‹œë¦¬ì–¼", "êµ¬ë¶„", "ëª¨ë¸ëª…", "ê¸°íƒ€" FROM assets ORDER BY id DESC')
    assets = [dict(row) for row in c.fetchall()]
    conn.close()
    return assets

def get_unique_statuses():
    conn = sqlite3.connect(ASSET_DB_PATH)
    c = conn.cursor()
    c.execute('SELECT DISTINCT "ì¥ë¹„ìƒíƒœ" FROM assets WHERE "ì¥ë¹„ìƒíƒœ" IS NOT NULL')
    statuses = [row[0] for row in c.fetchall()]
    conn.close()
    default_statuses = ["ì‚¬ìš©ì¤‘", "ë¯¸ì‚¬ìš©", "ìˆ˜ë¦¬ì¤‘", "íê¸°", "ì •ìƒ", "ë¹„ì •ìƒ"]
    unique_statuses = list(set(default_statuses + statuses))
    return unique_statuses

def update_asset(data):
    conn = sqlite3.connect(ASSET_DB_PATH)
    c = conn.cursor()
    try:
        c.execute('''
            UPDATE assets SET
            "ìì‚°ë²ˆí˜¸"=?, "ìœ„ì¹˜"=?, "ì¥ë¹„ìƒíƒœ"=?, "ì œì¡°ì‚¬"=?, "ì‹œë¦¬ì–¼"=?, "êµ¬ë¶„"=?, "ëª¨ë¸ëª…"=?, "ê¸°íƒ€"=?
            WHERE id=?
        ''', (data['ìì‚°ë²ˆí˜¸'], data['ìœ„ì¹˜'], data['ì¥ë¹„ìƒíƒœ'], data['ì œì¡°ì‚¬'], data['ì‹œë¦¬ì–¼'], data['êµ¬ë¶„'], data['ëª¨ë¸ëª…'], data['ê¸°íƒ€'], data['id']))
        conn.commit()
    except sqlite3.IntegrityError as e:
        st.error(f"DB ìˆ˜ì • ì˜¤ë¥˜: {e} (ìì‚°ë²ˆí˜¸: {data['ìì‚°ë²ˆí˜¸']})")
    except Exception as e:
        st.error(f"DB ìˆ˜ì • ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    finally:
        conn.close()

def delete_asset(asset_id):
    conn = sqlite3.connect(ASSET_DB_PATH)
    c = conn.cursor()
    try:
        c.execute("DELETE FROM assets WHERE id=?", (asset_id,))
        conn.commit()
    except Exception as e:
        st.error(f"DB ì‚­ì œ ì˜¤ë¥˜: {e}")
    finally:
        conn.close()

def delete_all_assets():
    conn = sqlite3.connect(ASSET_DB_PATH)
    c = conn.cursor()
    try:
        c.execute("DELETE FROM assets")
        c.execute("DELETE FROM sqlite_sequence WHERE name='assets'")
        conn.commit()
    except Exception as e:
        st.error(f"DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    finally:
        conn.close()

def get_existing_asset_numbers():
    conn = sqlite3.connect(ASSET_DB_PATH)
    c = conn.cursor()
    c.execute('SELECT "ìì‚°ë²ˆí˜¸" FROM assets')
    existing_numbers = [row[0] for row in c.fetchall()]
    conn.close()
    return set(existing_numbers)

def process_asset_file(uploaded_asset_file, overwrite=False):
    try:
        if uploaded_asset_file.name.endswith('.csv'):
            try:
                df = pd.read_csv(uploaded_asset_file, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(uploaded_asset_file, encoding='cp949')
        else:
            df = pd.read_excel(uploaded_asset_file)
        
        required_columns = ['ìì‚°ë²ˆí˜¸']
        if not all(col in df.columns for col in required_columns):
            st.error("íŒŒì¼ì— 'ìì‚°ë²ˆí˜¸' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìì‚°ë²ˆí˜¸ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
            return False
        
        existing_numbers = get_existing_asset_numbers()
        assets_to_add = df.to_dict('records')
        added_count, failed_count, duplicate_numbers = 0, 0, []
        valid_statuses = ["ì‚¬ìš©ì¤‘", "ë¯¸ì‚¬ìš©", "ìˆ˜ë¦¬ì¤‘", "íê¸°", "ì •ìƒ", "ë¹„ì •ìƒ"]
        
        for asset in assets_to_add:
            asset_data = {k: (v if pd.notna(v) else None) for k, v in asset.items()}
            if not asset_data.get('ìì‚°ë²ˆí˜¸'):
                st.warning(f"ìì‚°ë²ˆí˜¸ê°€ ëˆ„ë½ëœ í•­ëª© ë°œê²¬. í•´ë‹¹ í–‰ì€ ë¬´ì‹œë©ë‹ˆë‹¤.")
                failed_count += 1
                continue
            
            status = asset_data.get('ì¥ë¹„ìƒíƒœ')
            if status and status not in valid_statuses:
                st.warning(f"ì˜ëª»ëœ ìƒíƒœ ê°’ '{status}' ë°œê²¬ (ìì‚°ë²ˆí˜¸: {asset_data['ìì‚°ë²ˆí˜¸']}). 'ì‚¬ìš©ì¤‘'ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                asset_data['ì¥ë¹„ìƒíƒœ'] = 'ì‚¬ìš©ì¤‘'
            
            full_asset_data = {
                'ìì‚°ë²ˆí˜¸': asset_data.get('ìì‚°ë²ˆí˜¸'),
                'ìœ„ì¹˜': asset_data.get('ìœ„ì¹˜'),
                'ì¥ë¹„ìƒíƒœ': asset_data.get('ì¥ë¹„ìƒíƒœ'),
                'ì œì¡°ì‚¬': asset_data.get('ì œì¡°ì‚¬'),
                'ì‹œë¦¬ì–¼': asset_data.get('ì‹œë¦¬ì–¼'),
                'êµ¬ë¶„': asset_data.get('êµ¬ë¶„'),
                'ëª¨ë¸ëª…': asset_data.get('ëª¨ë¸ëª…'),
                'ê¸°íƒ€': asset_data.get('ê¸°íƒ€', asset_data.get('ì‹œìŠ¤í…œëª…'))
            }
            
            if full_asset_data['ìì‚°ë²ˆí˜¸'] in existing_numbers:
                if overwrite:
                    existing_asset = next((a for a in get_assets() if a['ìì‚°ë²ˆí˜¸'] == full_asset_data['ìì‚°ë²ˆí˜¸']), None)
                    if existing_asset:
                        full_asset_data['id'] = existing_asset['id']
                        update_asset(full_asset_data)
                        added_count += 1
                    else:
                        duplicate_numbers.append(full_asset_data['ìì‚°ë²ˆí˜¸'])
                        failed_count += 1
                else:
                    duplicate_numbers.append(full_asset_data['ìì‚°ë²ˆí˜¸'])
                    failed_count += 1
                continue
            
            if add_asset(full_asset_data):
                added_count += 1
            else:
                failed_count += 1
        
        if duplicate_numbers:
            st.warning(f"ë‹¤ìŒ ìì‚°ë²ˆí˜¸ëŠ” ì´ë¯¸ DBì— ì¡´ì¬í•˜ì—¬ ì¶”ê°€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(duplicate_numbers)}")
        if added_count > 0:
            st.success(f"ì´ {added_count}ê°œì˜ ìì‚°ì„ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        if failed_count > 0:
            st.error(f"{failed_count}ê°œì˜ ìì‚°ì€ ì¤‘ë³µëœ ìì‚°ë²ˆí˜¸ ë˜ëŠ” ê¸°íƒ€ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¶”ê°€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return added_count > 0
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def generate_qr_code(data):
    qr = qrcode.QRCode(version=1, box_size=10, border=5)
    qr.add_data(data)
    qr.make(fit=True)
    img = qr.make_image(fill='black', back_color='white')
    buf = BytesIO()
    img.save(buf)
    buf.seek(0)
    return buf

# --- êµ¬ê¸€ ìº˜ë¦°ë” ê¸°ëŠ¥ ---
def get_google_calendar_service(manual_auth=False):
    creds = None
    if os.path.exists(GOOGLE_TOKEN_PATH):
        with open(GOOGLE_TOKEN_PATH, 'rb') as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(Request())
            except Exception as e:
                st.warning(f"í† í° ê°±ì‹  ì‹¤íŒ¨: {e}. ì¬ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                creds = None
        else:
            if not os.path.exists(GOOGLE_CREDS_PATH):
                st.error(f"'{GOOGLE_CREDS_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OAuth 2.0 í´ë¼ì´ì–¸íŠ¸ ID íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return None
            try:
                flow = InstalledAppFlow.from_client_secrets_file(
                    GOOGLE_CREDS_PATH, scopes=['https://www.googleapis.com/auth/calendar.readonly']
                )
                auth_url, _ = flow.authorization_url(prompt='consent')
                st.info(f'ì•„ë˜ URLì— ì ‘ì†í•˜ì—¬ êµ¬ê¸€ ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•˜ê³ , í‘œì‹œë˜ëŠ” ì¸ì¦ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ ì•„ë˜ì— ë¶™ì—¬ë„£ì–´ ì£¼ì„¸ìš”.')
                st.code(auth_url)
                auth_code = st.text_input('ì¸ì¦ ì½”ë“œë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”:')
                if auth_code:
                    flow.fetch_token(code=auth_code)
                    creds = flow.credentials
                else:
                    return None
            except Exception as e:
                st.error(f"OAuth 2.0 ì¸ì¦ íë¦„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                if os.path.exists(GOOGLE_TOKEN_PATH):
                    os.remove(GOOGLE_TOKEN_PATH)
                return None
        with open(GOOGLE_TOKEN_PATH, 'wb') as token:
            pickle.dump(creds, token)
    try:
        service = build('calendar', 'v3', credentials=creds)
        return service
    except Exception as e:
        st.error(f"Google Calendar ì„œë¹„ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def get_calendar_events_from_query(query):
    try:
        today_str = datetime.now().strftime("%Y-%m-%d")
        chain_output = date_extraction_chain.invoke({"today": today_str, "question": query})
        extracted_info = json.loads(chain_output['text'])
        start_date_str = extracted_info.get("start_date", today_str)
        end_date_str = extracted_info.get("end_date", today_str)
        keyword = extracted_info.get("query", "ì „ì²´")
    except Exception as e:
        return f"ì§ˆë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì¢€ ë” ëª…í™•í•œ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
    service = get_google_calendar_service()
    if not service:
        return "ìº˜ë¦°ë” ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ì¦ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”."
    try:
        start_date_dt = datetime.fromisoformat(start_date_str)
        end_date_dt = datetime.fromisoformat(end_date_str)
        time_min = start_date_dt.isoformat() + 'Z'
        time_max = (end_date_dt + timedelta(days=1)).isoformat() + 'Z'
        calendar_list = service.calendarList().list().execute()
        all_events = []
        for calendar_entry in calendar_list.get('items', []):
            calendar_id = calendar_entry['id']
            events_result = service.events().list(
                calendarId=calendar_id, timeMin=time_min, timeMax=time_max,
                singleEvents=True, orderBy='startTime'
            ).execute()
            all_events.extend(events_result.get('items', []))
        all_events.sort(key=lambda x: x['start'].get('dateTime', x['start'].get('date')))
        events = all_events
        if keyword.lower() != 'ì „ì²´':
            search_terms = [term.strip().lower() for term in re.split(r'[,\s]+', keyword) if term.strip()]
            if search_terms:
                filtered_events = []
                for event in events:
                    summary = event.get('summary', '').lower()
                    if any(term in summary for term in search_terms):
                        filtered_events.append(event)
                events = filtered_events
            else:
                events = []
        if not events:
            return f"'{start_date_str}'ë¶€í„° '{end_date_str}'ê¹Œì§€ '{keyword}' ê´€ë ¨ ì¼ì •ì´ ì—†ìŠµë‹ˆë‹¤."
        response_text = f"ğŸ—“ï¸ **'{start_date_str} ~ {end_date_str}'ì˜ '{keyword}' ê´€ë ¨ ì¼ì •ì…ë‹ˆë‹¤.**\n\n"
        for event in events:
            start = event['start'].get('dateTime', event['start'].get('date'))
            start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))
            KST = timezone(timedelta(hours=9))
            kst_start_dt = start_dt.astimezone(KST)
            formatted_start = kst_start_dt.strftime('%Y-%m-%d %H:%M') if 'dateTime' in event['start'] else kst_start_dt.strftime('%Y-%m-%d') + " (ì¢…ì¼)"
            summary = event['summary']
            response_text += f"- **{summary}** ({formatted_start})\n"
        return response_text
    except Exception as e:
        return f"ì¼ì • ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# --- RAG ê¸°ëŠ¥ ---
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def parse_excel(file_path):
    try:
        if file_path.endswith('.csv'):
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding='cp949')
        else:
            df = pd.read_excel(file_path)
    except Exception as e:
        st.error(f"íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    chunks = []
    for _, row in df.iterrows():
        text = " | ".join([str(val) for val in row.values if pd.notna(val)])
        metadata = {"file": os.path.basename(file_path), "type": "excel"}
        for key, value in row.to_dict().items():
            if pd.notna(value):
                metadata[str(key)] = str(value)
        chunks.append({"text": text, "metadata": metadata})
    st.write(f"Parsed {len(chunks)} chunks from {file_path}")
    return chunks

def parse_docx(file_path):
    try:
        doc = Document(file_path)
    except Exception as e:
        st.error(f"DOCX íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    chunks = []
    for i, table in enumerate(doc.tables):
        table_text = ""
        for row in table.rows:
            row_text = " | ".join([cell.text.strip() for cell in row.cells])
            table_text += row_text + "\n"
        if table_text.strip():
            chunks.append({
                "text": table_text.strip(),
                "metadata": {"file": os.path.basename(file_path), "type": "docx_table", "table_index": i}
            })
    max_chunk_size = 500
    current_section = "ë³¸ë¬¸"
    for para in doc.paragraphs:
        text = para.text.strip()
        if not text:
            continue
        if text.startswith(("1.", "2.", "3.", "4.", "5.", "6.", "7.", "8.")):
            current_section = text.split()[0]
        while len(text) > max_chunk_size:
            split_point = text.rfind('.', 0, max_chunk_size)
            if split_point == -1:
                split_point = max_chunk_size
            chunk_text = text[:split_point]
            chunks.append({
                "text": chunk_text,
                "metadata": {"file": os.path.basename(file_path), "type": "docx_paragraph", "section": current_section}
            })
            text = text[split_point:].strip()
        if text:
            chunks.append({
                "text": text,
                "metadata": {"file": os.path.basename(file_path), "type": "docx_paragraph", "section": current_section}
            })
    st.write(f"Parsed {len(chunks)} chunks from {file_path}")
    return chunks

def parse_pdf(file_path):
    try:
        with pdfplumber.open(file_path) as pdf:
            chunks = []
            max_chunk_size = 500
            for page in pdf.pages:
                text = page.extract_text()
                tables = page.extract_tables()
                for table in tables:
                    table_text = "\n".join([" | ".join(str(cell) if cell else "" for cell in row) for row in table])
                    if table_text.strip():
                        chunks.append({
                            "text": table_text,
                            "metadata": {"file": os.path.basename(file_path), "type": "pdf_table", "page": page.page_number}
                        })
                if text:
                    while len(text) > max_chunk_size:
                        split_point = text.rfind('ã€‚', 0, max_chunk_size)
                        if split_point == -1: split_point = text.rfind('.', 0, max_chunk_size)
                        if split_point == -1: split_point = max_chunk_size
                        chunk_text = text[:split_point + 1]
                        chunks.append({
                            "text": chunk_text.strip(),
                            "metadata": {"file": os.path.basename(file_path), "type": "pdf_text", "page": page.page_number}
                        })
                        text = text[split_point + 1:].strip()
                    if text:
                        chunks.append({
                            "text": text,
                            "metadata": {"file": os.path.basename(file_path), "type": "pdf_text", "page": page.page_number}
                        })
    except Exception as e:
        st.error(f"PDF íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    st.write(f"Parsed {len(chunks)} chunks from {file_path}")
    return chunks

from pptx import Presentation
import streamlit as st
import os

def parse_pptx(file_path):
    try:
        prs = Presentation(file_path)
        chunks = []
        for slide in prs.slides:
            slide_text = ""
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    slide_text += shape.text + "\n"
            if slide_text.strip():
                chunks.append({
                    "text": slide_text.strip(),
                    "metadata": {"file": os.path.basename(file_path), "type": "pptx_slide", "slide_number": prs.slides.index(slide) + 1}
                })
    except Exception as e:
        st.error(f"PPTX íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    st.write(f"Parsed {len(chunks)} chunks from {file_path}")
    return chunks

def parse_txt(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        chunks = []
        max_chunk_size = 500
        while len(text) > max_chunk_size:
            split_point = text.rfind('ã€‚', 0, max_chunk_size)
            if split_point == -1: split_point = text.rfind('.', 0, max_chunk_size)
            if split_point == -1: split_point = max_chunk_size
            chunk_text = text[:split_point + 1]
            chunks.append({"text": chunk_text.strip(), "metadata": {"file": os.path.basename(file_path), "type": "txt"}})
            text = text[split_point + 1:].strip()
        if text:
            chunks.append({"text": text, "metadata": {"file": os.path.basename(file_path), "type": "txt"}})
    except Exception as e:
        st.error(f"TXT íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    st.write(f"Parsed {len(chunks)} chunks from {file_path}")
    return chunks

def parse_image(file_path):

    try:
        img = Image.open(file_path)
        img = img.convert('L')
        img = ImageEnhance.Contrast(img).enhance(2.0)
        img = img.filter(ImageFilter.SHARPEN)
        text = pytesseract.image_to_string(img, lang='kor+eng', config='--psm 6')
        if not text.strip():
            st.warning(f"{file_path}ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        chunks = []
        max_chunk_size = 500
        while len(text) > max_chunk_size:
            split_point = text.rfind('ã€‚', 0, max_chunk_size)
            if split_point == -1: split_point = text.rfind('.', 0, max_chunk_size)
            if split_point == -1: split_point = max_chunk_size
            chunk_text = text[:split_point + 1]
            chunks.append({"text": chunk_text.strip(), "metadata": {"file": os.path.basename(file_path), "type": "image"}})
            text = text[split_point + 1:].strip()
        if text:
            chunks.append({"text": text, "metadata": {"file": os.path.basename(file_path), "type": "image"}})
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    st.write(f"Parsed {len(chunks)} chunks from {file_path}")
    return chunks

def embed_and_store(chunks):
    if not chunks:
        return
    co_client, faiss_index, _, current_metadata = get_clients()
    docs_to_add = [chunk["text"] for chunk in chunks]
    metadatas_to_add = [chunk for chunk in chunks]
    try:
        embeddings = co_client.embed(texts=docs_to_add, model="embed-multilingual-v3.0", input_type="search_document").embeddings
        embeddings = np.array(embeddings).astype('float32')
        faiss_index.add(embeddings)
        current_metadata.extend(metadatas_to_add)
        faiss.write_index(faiss_index, FAISS_INDEX_PATH)
        with open(FAISS_METADATA_PATH, 'wb') as f:
            pickle.dump(current_metadata, f)
        st.success(f"Stored {len(chunks)} chunks in FAISS")
    except Exception as e:
        st.error(f"ì„ë² ë”© ì €ì¥ ì˜¤ë¥˜: {e}")

def hybrid_retriever(query, top_k=5):
    co_client, faiss_index, _, current_metadata = get_clients()
    try:
        query_embedding = co_client.embed(texts=[query], model="embed-multilingual-v3.0", input_type="search_query").embeddings[0]
        query_embedding = np.array([query_embedding]).astype('float32')
        distances, indices = faiss_index.search(query_embedding, top_k * 2)
        retrieved_docs = []
        retrieved_metadatas = []
        for idx in indices[0]:
            if idx < len(current_metadata) and 'text' in current_metadata[idx]:
                retrieved_docs.append(current_metadata[idx]["text"])
                retrieved_metadatas.append(current_metadata[idx]["metadata"])
        if not retrieved_docs:
            st.warning("No documents found for reranking.")
            return [], []
        rerank_results = co_client.rerank(
            query=query, 
            documents=retrieved_docs, 
            top_n=top_k, 
            model="rerank-multilingual-v3.0",
            return_documents=True
        )
        reranked_docs = []
        reranked_metadatas = []
        if rerank_results.results:
            for result in rerank_results.results:
                if result.document:
                    reranked_docs.append(result.document.text)
                    reranked_metadatas.append(retrieved_metadatas[result.index])
        st.write(f"Retrieved {len(reranked_docs)} documents after reranking")
        return reranked_docs, reranked_metadatas
    except Exception as e:
        st.error(f"ë¦¬íŠ¸ë¦¬ë²„ ì˜¤ë¥˜: {e}")
        return [], []

def generate_narrative_response(query, docs, metadatas):
    context = "\n\n---\n\n".join(docs)
    prompt = f"""
    ë‹¹ì‹ ì€ ì œê³µëœ 'ë¬¸ì„œ'ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì„œìˆ í˜•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    # ì§€ì‹œì‚¬í•­
    1. **ì§ˆë¬¸ íŒŒì•…:** ì‚¬ìš©ìì˜ ì§ˆë¬¸('{query}')ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•©ë‹ˆë‹¤.
    2. **ë¬¸ì„œ ë¶„ì„:** ì œê³µëœ 'ë¬¸ì„œ' ë‚´ìš©ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
    3. **ë‹µë³€ ìƒì„±:**
        - ë¶„ì„í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        - ë”±ë”±í•œ í˜•ì‹ì´ ì•„ë‹Œ, ì‚¬ëŒì´ ì„¤ëª…í•´ì£¼ë“¯ì´ ì¹œì ˆí•œ ì–´ì¡°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        - í•„ìš”í•˜ë‹¤ë©´, ì •ë³´ë¥¼ ëª©ë¡ì´ë‚˜ ë‹¨ë½ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
    4. **ì¶”ì¸¡ ê¸ˆì§€:** ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš°, "ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œìœ¼ë¡œëŠ” ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤." ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

    # ë¬¸ì„œ (ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”)
    ---
    {context}
    ---

    # ì§ˆë¬¸
    {query}

    # ë‹µë³€
    """
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that provides detailed, narrative answers based on provided documents."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"GPT-4o í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def classify_query_intent(query):
    prompt = f"""
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ë‹µë³€ì´ 'í…Œì´ë¸”(í‘œ)' í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ì–´ì•¼ í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ 'ì„œìˆ í˜•'ìœ¼ë¡œ ì œê³µë˜ì–´ì•¼ í•˜ëŠ”ì§€ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

    # ì§€ì‹œì‚¬í•­
    1. **'table'ë¡œ ë¶„ë¥˜í•´ì•¼ í•˜ëŠ” ê²½ìš° (ë§¤ìš° ì œí•œì ):**
        - ëª…ì‹œì ìœ¼ë¡œ 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'í˜„í™©', 'ì¥ë¹„', 'í‘œ', 'í…Œì´ë¸”' ë“± í‘œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°.
        - ì—¬ëŸ¬ í•­ëª©ì— ëŒ€í•œ ë¹„êµë‚˜ ìš”ì•½ì„ ìš”ì²­í•˜ë©°, ê·¸ ê²°ê³¼ê°€ ëª…í™•íˆ êµ¬ì¡°í™”ëœ ë°ì´í„° í˜•íƒœì¼ ë•Œ.
        - ì˜ˆì‹œ: "A ì¥ë¹„ ëª©ë¡ ë³´ì—¬ì¤˜", "HRSì½”ë¦¬ì•„ ìœ ì§€ë³´ìˆ˜ í˜„í™© ì•Œë ¤ì¤˜", "ëª¨ë“  ìì‚° ë¦¬ìŠ¤íŠ¸", "2024ë…„ ë„¤íŠ¸ì›Œí¬ ì¥ë¹„ ëª©ë¡", "ì´ ë°ì´í„°ë¥¼ í‘œë¡œ ë³´ì—¬ì¤˜"

    2. **'narrative'ë¡œ ë¶„ë¥˜í•´ì•¼ í•˜ëŠ” ê²½ìš° (ê¸°ë³¸ê°’):**
        - ì„¤ëª…, ë°©ë²•, ì´ìœ , ì •ì˜ ë“± ì„œìˆ ì ì¸ ë‹µë³€ì´ í•„ìš”í•œ ëª¨ë“  ì§ˆë¬¸.
        - ë‹¨ì¼ ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸.
        - ì¼ë°˜ì ì¸ ëŒ€í™”í˜• ì§ˆë¬¸.
        - 'table'ë¡œ ë¶„ë¥˜ë  ëª…í™•í•œ ê·¼ê±°ê°€ ì—†ëŠ” ëª¨ë“  ì§ˆë¬¸ì€ 'narrative'ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
        - ì˜ˆì‹œ: "PAS-Kê°€ ë­ì•¼?", "ì™€ì´íŒŒì´ ì ‘ì† ì–´ë–»ê²Œ í•´?", "ì–´ì œ ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?", "ë°±ì¢…ìœ¤ì˜ ì¼ì •ì´ ì–´ë–»ê²Œ ë¼?", "ì´ ë¬¸ì„œì˜ ìš”ì•½ì€?", "ì´ ê°œë…ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"

    3. **ì¶œë ¥ í˜•ì‹:**
        - ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì˜¤ì§ 'table' ë˜ëŠ” 'narrative' ë‹¨ì–´ í•˜ë‚˜ë¡œë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

    # ì§ˆë¬¸
    {query}

    # ë¶„ë¥˜ ê²°ê³¼
    """
    try:
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a query classifier. Your only output should be 'table' or 'narrative'."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=5
        )
        classification = response.choices[0].message.content.strip().lower()
        if classification not in ['table', 'narrative']:
            return 'narrative'
        return classification
    except Exception as e:
        st.error(f"Query classification error: {e}")
        return 'narrative'

def generate_response(query, docs, metadatas):
    context = "\n\n---\n\n".join(docs)
    prompt = f"""
    ë‹¹ì‹ ì€ ì œê³µëœ 'ë¬¸ì„œ'ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.

    # ì§€ì‹œì‚¬í•­
    1. **ì§ˆë¬¸ ë¶„ì„:** ì‚¬ìš©ìì˜ ì§ˆë¬¸('{query}')ì„ ì •í™•íˆ ì´í•´í•©ë‹ˆë‹¤.
    2. **ë°ì´í„° ì¶”ì¶œ:** 'ë¬¸ì„œ' ë‚´ìš© ì „ì²´ë¥¼ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ì—¬, ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ë¬¸ì„œê°€ CSV í‘œ í˜•ì‹ì¸ ê²½ìš°, ëª¨ë“  í–‰(row)ì„ ë¹ ì§ì—†ì´ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
    3. **JSON í˜•ì‹í™”:**
        - ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        - JSONì˜ ìµœìƒìœ„ í‚¤ëŠ” 'data' ì—¬ì•¼ í•˜ê³ , ê·¸ ê°’ì€ ê°ì²´ë“¤ì˜ ë°°ì—´(list of objects)ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        - ê° ê°ì²´ëŠ” ë¬¸ì„œì˜ í•œ í–‰(row)ì— í•´ë‹¹í•˜ë©°, ë¬¸ì„œì˜ í—¤ë”(header)ë¥¼ í‚¤(key)ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        - ì˜ˆì‹œ: {{"data": [{{"column1": "value1", "column2": "value2"}}, {{"column1": "value3", "column2": "value4"}}]}}
    4. **ì™„ì „ì„±:** ì§ˆë¬¸ì´ 'ì „ì²´ ëª©ë¡'ì´ë‚˜ 'ëª¨ë“  í’ˆëª©'ì„ ìš”êµ¬í•˜ëŠ” ê²½ìš°, ë‹¨ í•˜ë‚˜ì˜ ë°ì´í„°ë„ ëˆ„ë½í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  ê´€ë ¨ ë°ì´í„°ë¥¼ JSONì— í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
    5. **ì¶”ì¸¡ ê¸ˆì§€:** ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ì •ë³´ê°€ ì—†ìœ¼ë©´ {{"data": []}} ì™€ ê°™ì´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    # ë¬¸ì„œ (ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ JSONì„ ìƒì„±í•˜ì„¸ìš”)
    ---
    {context}
    ---
    """
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a precise AI that extracts information from documents and returns it in JSON format."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.0,
            max_tokens=16000
        )
        st.session_state.json_response = response.choices[0].message.content
        return st.session_state.json_response
    except Exception as e:
        st.error(f"GPT-4o í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None

# --- Streamlit UI ---
st.title("ì„œë¹„ìŠ¤ì‚¬ì—…ë¶€ ì±—ë´‡ ì‹œìŠ¤í…œ")
init_asset_db()
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ì—…ë¬´ì¡°íšŒ", "ì—…ë¬´ì¼ì •", "ìì‚°í˜„í™©", "ì—…ë¬´í•™ìŠµ", "ë°±í„°DB", "ì¥ë¹„ê´€ë¦¬"])

# --- ì—…ë¬´ì¡°íšŒ íƒ­ ---
with tab1:
    st.header("AI ì—…ë¬´ ì±—ë´‡")
    with st.form(key="query_form"):
        query = st.text_input("ìœ ì§€ë³´ìˆ˜ ì—…ë¬´ì™€ ê´€ë ¨í•˜ì—¬ ë¬¼ì–´ë³´ì„¸ìš” (ì˜ˆ: í•œêµ­êµ­ì œí˜‘ë ¥ë‹¨ ìœ ì§€ë³´ìˆ˜ í˜„í™©):", key="query")
        submit_button = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°")
    if submit_button and query:
        with st.spinner("ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            docs, metadatas = hybrid_retriever(query)
            if not docs:
                st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                intent = classify_query_intent(query)
                if intent == 'table':
                    st.info("ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ 'í…Œì´ë¸”'ë¡œ íŒŒì•…í–ˆìŠµë‹ˆë‹¤. í‘œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    json_response_str = generate_response(query, docs, metadatas)
                    if json_response_str:
                        try:
                            response_data = json.loads(json_response_str)
                            data = response_data.get('data', [])
                            if data and isinstance(data, list):
                                st.subheader("ê²€ìƒ‰ ê²°ê³¼")
                                df = pd.DataFrame(data)
                                st.dataframe(df, use_container_width=True, hide_index=True)
                            else:
                                st.warning("í‘œë¡œ ë§Œë“¤ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëŒ€ì‹  ì„œìˆ í˜• ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                                narrative_response = generate_narrative_response(query, docs, metadatas)
                                st.subheader("AI ì‘ë‹µ")
                                st.markdown(narrative_response)
                        except json.JSONDecodeError:
                            st.error("JSON ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì‘ë‹µì„ í‘œì‹œí•©ë‹ˆë‹¤.")
                            st.write(json_response_str)
                    else:
                        st.warning("AIë¡œë¶€í„° í…Œì´ë¸” í˜•ì‹ì˜ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.info("ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ 'ì„œìˆ í˜•'ìœ¼ë¡œ íŒŒì•…í–ˆìŠµë‹ˆë‹¤. ëŒ€í™” í˜•íƒœë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    narrative_response = generate_narrative_response(query, docs, metadatas)
                    st.subheader("AI ì‘ë‹µ")
                    st.markdown(narrative_response)
    elif submit_button and not query:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# --- ì—…ë¬´ì¼ì • íƒ­ ---
with tab2:
    st.header("ğŸ“… ì—…ë¬´ì¼ì • ì¡°íšŒ")
    if not os.path.exists(GOOGLE_TOKEN_PATH):
        st.info("ì—…ë¬´ì¼ì • ì¡°íšŒë¥¼ ìœ„í•´ êµ¬ê¸€ ìº˜ë¦°ë” ì—°ë™ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        if st.button("êµ¬ê¸€ ìº˜ë¦°ë” ì—°ë™ ë° ì¸ì¦"):
            with st.spinner("ì¸ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤. í„°ë¯¸ë„ì˜ ì•ˆë‚´ì— ë”°ë¼ ë¸Œë¼ìš°ì €ì—ì„œ ì¸ì¦ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”..."):
                get_google_calendar_service(manual_auth=True)
                st.success("ì¸ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì¼ì •ì„ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                st.rerun()
    else:
        with st.form(key="schedule_form"):
            schedule_query = st.text_input("ì—…ë¬´ ì¼ì •ì„ ìì—°ì–´ë¡œ ë¬¼ì–´ë³´ì„¸ìš”:", placeholder="ì˜ˆ: 8ì›” 1ì¼ ~ 8ì›” 10ì¼ í˜„ìš°, ì›ì¤€ ì¼ì •")
            schedule_submit_button = st.form_submit_button("ì¼ì • ê²€ìƒ‰")
        if schedule_submit_button and schedule_query:
            with st.spinner("ì¼ì •ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                events_response = get_calendar_events_from_query(schedule_query)
                st.markdown(events_response)

# --- ìì‚°í˜„í™© íƒ­ ---
with tab3:
    st.header("ğŸ“‹ ìì‚° í˜„í™© ë° ê´€ë¦¬")
    st.subheader("ìì‚° ëª©ë¡")
    assets = get_assets()
    if assets:
        df = pd.DataFrame(assets)
        st.dataframe(df, use_container_width=True, hide_index=True)
    else:
        st.info("ë“±ë¡ëœ ìì‚°ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    with st.expander("â• ìƒˆ ìì‚° ì¶”ê°€í•˜ê¸°"):
        with st.form(key="asset_add_form", clear_on_submit=True):
            c1, c2 = st.columns(2)
            asset_data = {
                "ìì‚°ë²ˆí˜¸": c1.text_input("ìì‚°ë²ˆí˜¸*"),
                "ìœ„ì¹˜": c2.text_input("ìœ„ì¹˜"),
                "ì¥ë¹„ìƒíƒœ": c1.selectbox("ì¥ë¹„ìƒíƒœ", ["ì‚¬ìš©ì¤‘", "ë¯¸ì‚¬ìš©", "ìˆ˜ë¦¬ì¤‘", "íê¸°", "ì •ìƒ", "ë¹„ì •ìƒ"]),
                "ì œì¡°ì‚¬": c2.text_input("ì œì¡°ì‚¬"),
                "ì‹œë¦¬ì–¼": c1.text_input("ì‹œë¦¬ì–¼"),
                "êµ¬ë¶„": c2.text_input("êµ¬ë¶„ (ì˜ˆ: ì„œë²„, ë…¸íŠ¸ë¶)"),
                "ëª¨ë¸ëª…": c1.text_input("ëª¨ë¸ëª…"),
                "ê¸°íƒ€": c2.text_input("ê¸°íƒ€")
            }
            if st.form_submit_button("ìì‚° ì¶”ê°€"):
                if not asset_data["ìì‚°ë²ˆí˜¸"]:
                    st.error("ìì‚°ë²ˆí˜¸ëŠ” í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤.")
                else:
                    if add_asset(asset_data):
                        st.success("ìì‚°ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.rerun()

    with st.expander("âœï¸ ìì‚° ìˆ˜ì • ë˜ëŠ” ì‚­ì œí•˜ê¸°"):
        if assets:
            selected_id = st.selectbox("ê´€ë¦¬í•  ìì‚°ì˜ IDë¥¼ ì„ íƒí•˜ì„¸ìš”.", options=[a['id'] for a in assets], format_func=lambda x: f"ID: {x}")
            selected_asset = next((a for a in assets if a['id'] == selected_id), None)
            if selected_asset:
                col1, col2 = st.columns([3, 1])
                with col1:
                    with st.form(key=f"edit_form_{selected_id}"):
                        st.write(f"**ID {selected_id} ìì‚° ì •ë³´ ìˆ˜ì •**")
                        status_options = get_unique_statuses()
                        current_status = selected_asset['ì¥ë¹„ìƒíƒœ'] if selected_asset['ì¥ë¹„ìƒíƒœ'] else "ì‚¬ìš©ì¤‘"
                        if current_status not in status_options:
                            status_options.append(current_status)
                        try:
                            status_index = status_options.index(current_status)
                        except ValueError:
                            status_index = 0
                            st.warning(f"ìƒíƒœ '{current_status}'ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 'ì‚¬ìš©ì¤‘'ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.")
                        updated_data = {
                            "id": selected_id,
                            "ìì‚°ë²ˆí˜¸": st.text_input("ìì‚°ë²ˆí˜¸", value=selected_asset['ìì‚°ë²ˆí˜¸']),
                            "ìœ„ì¹˜": st.text_input("ìœ„ì¹˜", value=selected_asset['ìœ„ì¹˜']),
                            "ì¥ë¹„ìƒíƒœ": st.selectbox("ì¥ë¹„ìƒíƒœ", status_options, index=status_index),
                            "ì œì¡°ì‚¬": st.text_input("ì œì¡°ì‚¬", value=selected_asset['ì œì¡°ì‚¬']),
                            "ì‹œë¦¬ì–¼": st.text_input("ì‹œë¦¬ì–¼", value=selected_asset['ì‹œë¦¬ì–¼']),
                            "êµ¬ë¶„": st.text_input("êµ¬ë¶„", value=selected_asset['êµ¬ë¶„']),
                            "ëª¨ë¸ëª…": st.text_input("ëª¨ë¸ëª…", value=selected_asset['ëª¨ë¸ëª…']),
                            "ê¸°íƒ€": st.text_input("ê¸°íƒ€", value=selected_asset['ê¸°íƒ€'])
                        }
                        if st.form_submit_button("ìˆ˜ì • ì™„ë£Œ"):
                            update_asset(updated_data)
                            st.success(f"ID {selected_id} ìì‚° ì •ë³´ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                with col2:
                    st.write("**QR ì½”ë“œ & ì‚­ì œ**")
                    qr_data_str = ", ".join([f"{k}: {v}" for k, v in selected_asset.items() if k != 'id'])
                    qr_img = generate_qr_code(qr_data_str)
                    st.image(qr_img, caption="ìì‚° ì •ë³´ QR ì½”ë“œ")
                    if st.button("ì´ ìì‚° ì‚­ì œ", key=f"delete_{selected_id}", type="primary"):
                        delete_asset(selected_id)
                        st.warning(f"ID {selected_id} ìì‚°ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
        else:
            st.info("ìˆ˜ì • ë˜ëŠ” ì‚­ì œí•  ìì‚°ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    st.markdown("---")
    st.subheader("ğŸ—‚ï¸ ìì‚° ì¼ê´„ ë“±ë¡ ë° DB ê´€ë¦¬")
    if 'asset_upload_processed' not in st.session_state:
        st.session_state.asset_upload_processed = False
    overwrite_option = st.checkbox("ì¤‘ë³µ ìì‚°ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°")
    uploaded_asset_file = st.file_uploader("ìì‚° ëª©ë¡ ì—‘ì…€/CSV íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx", "csv"], key="asset_uploader")
    if uploaded_asset_file and not st.session_state.asset_upload_processed:
        with st.spinner("íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ìì‚°ì„ ë“±ë¡í•˜ëŠ” ì¤‘..."):
            success = process_asset_file(uploaded_asset_file, overwrite=overwrite_option)
            if success:
                st.session_state.asset_upload_processed = True
                st.rerun()
            else:
                st.session_state.asset_upload_processed = False
    if st.session_state.asset_upload_processed:
        st.session_state.asset_upload_processed = False

    if 'confirm_asset_reset' not in st.session_state:
        st.session_state.confirm_asset_reset = False
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ìì‚° DB ì´ˆê¸°í™”", type="primary"):
        st.session_state.confirm_asset_reset = True
    if st.session_state.confirm_asset_reset:
        st.warning("ëª¨ë“  ìì‚° ë°ì´í„°ê°€ ì˜êµ¬ì ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", icon="âš ï¸")
        c1, c2 = st.columns(2)
        if c1.button("ì˜ˆ, ì´ˆê¸°í™”í•©ë‹ˆë‹¤.", key="confirm_asset_delete"):
            try:
                delete_all_assets()
                st.session_state.confirm_asset_reset = False
                st.success("ëª¨ë“  ìì‚° ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
            except Exception as e:
                st.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.session_state.confirm_asset_reset = False
        if c2.button("ì•„ë‹ˆìš”, ì·¨ì†Œí•©ë‹ˆë‹¤.", key="cancel_asset_delete"):
            st.session_state.confirm_asset_reset = False
            st.rerun()

# --- ì—…ë¬´í•™ìŠµ íƒ­ ---
with tab4:
    st.header("ì—…ë¬´í•™ìŠµ")
    uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”(ì˜ˆ: CSV, DOCX, PDF, JPG, PNG, PPT, TXT..)", type=["csv", "docx", "pdf", "jpg", "jpeg", "png", "ppt", "pptx", "txt"])
    if uploaded_file is not None:
        if uploaded_file.size > MAX_FILE_SIZE:
            st.error("íŒŒì¼ í¬ê¸°ëŠ” 16MBë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        elif allowed_file(uploaded_file.name):
            file_path = os.path.join(UPLOAD_FOLDER, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            try:
                if uploaded_file.name.endswith('.csv'): chunks = parse_excel(file_path)
                elif uploaded_file.name.endswith('.docx'): chunks = parse_docx(file_path)
                elif uploaded_file.name.endswith('.pdf'): chunks = parse_pdf(file_path)
                elif uploaded_file.name.endswith(('.jpg', '.jpeg', 'png')): chunks = parse_image(file_path)
                elif uploaded_file.name.endswith(('.ppt', '.pptx')): chunks = parse_pptx(file_path)
                elif uploaded_file.name.endswith('.txt'): chunks = parse_txt(file_path)
                else:
                    st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
                    chunks = []
                if chunks:
                    embed_and_store(chunks)
                    st.success(f"{uploaded_file.name} ì—…ë¡œë“œ ë° ì²˜ë¦¬ ì™„ë£Œ!")
                else:
                    st.error("íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            st.error("CSV, DOCX, PDF, JPG, PNG íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# --- ë°±í„°DB íƒ­ ---
with tab5:
    st.header("ğŸ—„ï¸ ìì‚°ê´€ë¦¬ ë°±í„°DB")
    if 'confirm_vector_db_reset' not in st.session_state:
        st.session_state.confirm_vector_db_reset = False
    if st.button("ìì‚°ê´€ë¦¬ DB ì´ˆê¸°í™”", help="í•™ìŠµëœ ëª¨ë“  ë¬¸ì„œ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.", type="primary"):
        st.session_state.confirm_vector_db_reset = True
    if st.session_state.confirm_vector_db_reset:
        st.warning("ëª¨ë“  FAISS ë²¡í„° ë°ì´í„°ê°€ ì˜êµ¬ì ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", icon="âš ï¸")
        c1, c2 = st.columns(2)
        if c1.button("ì˜ˆ, ì´ˆê¸°í™”í•©ë‹ˆë‹¤.", key="confirm_vector_db_delete"):
            try:
                dimension = 1024
                new_index = faiss.IndexFlatL2(dimension)
                new_metadata = []
                faiss.write_index(new_index, FAISS_INDEX_PATH)
                with open(FAISS_METADATA_PATH, 'wb') as f:
                    pickle.dump(new_metadata, f)
                get_clients.clear()
                st.success("FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ!")
                st.session_state.confirm_vector_db_reset = False
                st.rerun()
            except Exception as e:
                st.error(f"FAISS ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
                st.session_state.confirm_vector_db_reset = False
        if c2.button("ì•„ë‹ˆìš”, ì·¨ì†Œí•©ë‹ˆë‹¤.", key="cancel_vector_db_delete"):
            st.session_state.confirm_vector_db_reset = False
            st.rerun()
    _, _, _, current_metadata = get_clients()
    if current_metadata:
        st.subheader("ì €ì¥ëœ ë¬¸ì„œ Chunk ëª©ë¡")
        doc_count = len(current_metadata)
        st.write(f"ì´ {doc_count}ê°œì˜ ë¬¸ì„œ Chunkê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ìµœëŒ€ 10ê°œ í‘œì‹œ)")
        for i, meta in enumerate(current_metadata):
            if i >= 10:
                break
            if not isinstance(meta, dict) or 'text' not in meta:
                st.warning(f"Chunk {i+1}ì— ì˜ëª»ëœ ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ìˆìŠµë‹ˆë‹¤: {meta}")
                continue
            with st.expander(f"**Chunk {i+1}**: {meta['text'][:80]}..."):
                st.write(f"**ì „ì²´ ë‚´ìš©**: {meta['text']}")
                st.write(f"**ë©”íƒ€ë°ì´í„°**: {meta['metadata']}")
    else:
        st.info("ì €ì¥ëœ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

# --- ì¥ë¹„ê´€ë¦¬ íƒ­ ---
with tab6:
    st.header("ğŸ“¡ ë„¤íŠ¸ì›Œí¬ ì¥ë¹„ ê´€ë¦¬")
    subtab1, subtab2 = st.tabs(["ë°ì´í„° ìˆ˜ì§‘ ë° ì„¤ì •", "ë„¤íŠ¸ì›Œí¬ ì±—ë´‡"])
    
    with subtab1:
        st.subheader("âš™ï¸ ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •")
        with st.expander("1. ì¥ë¹„ ì ‘ì† ì •ë³´", expanded=True):
            network_host = st.text_input("Host IP", value="192.168.9.19", key="network_host")
            network_username = st.text_input("Username", value="inner", key="network_username")
            network_password = st.text_input("Password", type="password", value="", key="network_password")
            network_secret = st.text_input("Enable Secret", type="password", value="", key="network_secret")
            network_port = st.number_input("SSH Port", min_value=1, max_value=65535, value=22, key="network_port")
            ssh_key_file = st.text_input("SSH Key File Path (Optional)", placeholder="/path/to/your/private_key", key="ssh_key_file")
        
        with st.expander("2. ëª…ë ¹ì–´ ëª©ë¡ ì„¤ì •"):
            uploaded_file = st.file_uploader("commands.txt íŒŒì¼ ì—…ë¡œë“œ", type="txt", key="network_commands_uploader")
            default_commands = [
                "show version", "show running-config", "show log", "show ip interface brief",
                "show vlan", "show interface status", "show arp", "show mac address-table"
            ]
            if uploaded_file is not None:
                try:
                    commands_to_run = uploaded_file.getvalue().decode("utf-8").splitlines()
                    st.session_state.network_commands = [cmd for cmd in commands_to_run if cmd.strip()]
                except Exception as e:
                    st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                    st.session_state.network_commands = default_commands
            else:
                st.session_state.network_commands = st.session_state.get('network_commands', default_commands)
            st.text_area("ì ìš©ë  ëª…ë ¹ì–´ ëª©ë¡", value='\n'.join(st.session_state.network_commands), height=200, disabled=True)
        
        if st.button("ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ë° ì¸ë±ìŠ¤ ìƒì„±", type="primary"):
            # ë°ì´í„° ìˆ˜ì§‘ ì „ í•­ìƒ ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ ì‚­ì œ
            try:
                if os.path.exists(NETWORK_FAISS_INDEX_PATH):
                    import shutil
                    shutil.rmtree(NETWORK_FAISS_INDEX_PATH)
                if os.path.exists(NETWORK_RAW_DATA_FILE):
                    os.remove(NETWORK_RAW_DATA_FILE)
                load_network_rag_chain.clear()
                st.toast("ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ ë²¡í„° DBë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ DB ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
                st.stop()

            device_info = {
                "device_type": "cisco_ios",
                "host": st.session_state.network_host,
                "username": st.session_state.network_username,
                "password": st.session_state.network_password,
                "secret": st.session_state.network_secret,
                "port": st.session_state.network_port,
            }
            if st.session_state.ssh_key_file:
                device_info['use_keys'] = True
                device_info['key_file'] = st.session_state.ssh_key_file
            with st.spinner("ì¥ë¹„ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ì¤‘..."):
                success, message = fetch_network_data(device_info, st.session_state.network_commands)
                if success:
                    success, message = build_network_vector_store()
            if success:
                st.success("ë°ì´í„° ìˆ˜ì§‘ ë° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                if 'network_messages' in st.session_state:
                    del st.session_state['network_messages']
                load_network_rag_chain.clear()
                st.rerun()
            else:
                st.error(message)
        
        if 'confirm_network_db_reset' not in st.session_state:
            st.session_state.confirm_network_db_reset = False
        if st.button("ë„¤íŠ¸ì›Œí¬ ë°±í„°DB ì´ˆê¸°í™”", help="ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.", type="primary"):
            st.session_state.confirm_network_db_reset = True
        if st.session_state.confirm_network_db_reset:
            st.warning("ëª¨ë“  ë„¤íŠ¸ì›Œí¬ FAISS ë²¡í„° ë°ì´í„°ê°€ ì˜êµ¬ì ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", icon="âš ï¸")
            c1, c2 = st.columns(2)
            if c1.button("ì˜ˆ, ì´ˆê¸°í™”í•©ë‹ˆë‹¤.", key="confirm_network_db_delete"):
                try:
                    if os.path.exists(NETWORK_FAISS_INDEX_PATH):
                        import shutil
                        shutil.rmtree(NETWORK_FAISS_INDEX_PATH)
                    if os.path.exists(NETWORK_RAW_DATA_FILE):
                        os.remove(NETWORK_RAW_DATA_FILE)
                    load_network_rag_chain.clear()
                    st.success("ë„¤íŠ¸ì›Œí¬ FAISS ì¸ë±ìŠ¤ ë° ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ë„¤íŠ¸ì›Œí¬ FAISS ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
                finally:
                    st.session_state.confirm_network_db_reset = False
                    st.rerun()
            if c2.button("ì•„ë‹ˆìš”, ì·¨ì†Œí•©ë‹ˆë‹¤.", key="cancel_network_db_delete"):
                st.session_state.confirm_network_db_reset = False
                st.rerun()
    
    with subtab2:
        st.subheader("ğŸ¤– ë„¤íŠ¸ì›Œí¬ ì±—ë´‡")
        if not os.path.exists(NETWORK_FAISS_INDEX_PATH):
            st.info("ì•„ì§ ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ë°ì´í„° ìˆ˜ì§‘ ë° ì„¤ì •' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
        else:
            qa_chain = load_network_rag_chain()
            if not qa_chain:
                st.error("ì±—ë´‡ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë‹¤ì‹œ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
            else:
                if "network_messages" not in st.session_state:
                    st.session_state.network_messages = []
                for message in st.session_state.network_messages:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])
                if prompt := st.chat_input("ì˜ˆ: VLAN 9ë²ˆì— í• ë‹¹ëœ í¬íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"):
                    st.session_state.network_messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"):
                        st.markdown(prompt)
                    with st.chat_message("assistant"):
                        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                            response = qa_chain.invoke({"query": prompt})
                            answer = response['result']
                            st.markdown(answer)
                            st.session_state.network_messages.append({"role": "assistant", "content": answer})
        
        st.markdown("---")
        st.subheader("ğŸ“„ ìˆ˜ì§‘ëœ ë„¤íŠ¸ì›Œí¬ Raw Data")
        try:
            with open(NETWORK_RAW_DATA_FILE, "r", encoding="utf-8") as f:
                raw_data = f.read()
            st.code(raw_data, language="log")
        except FileNotFoundError:
            st.warning("ì•„ì§ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ìˆ˜ì§‘ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"ë¡œìš° ë°ì´í„°ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    